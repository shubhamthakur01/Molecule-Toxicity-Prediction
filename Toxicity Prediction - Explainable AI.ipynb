{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbuxZkdJITeW",
    "outputId": "fe9f85dd-782b-4295-f68d-8e54d5c744cb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning \n",
    "import collections\n",
    "import copy\n",
    "import json\n",
    "from math import sqrt\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import GridSearchCV , cross_val_score\n",
    "from treeinterpreter import treeinterpreter as ti\n",
    "from functools import partial\n",
    "import keras\n",
    "from keras.models import load_model, model_from_json\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore',category = DataConversionWarning)\n",
    "warnings.filterwarnings(action = 'ignore',category = FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80RGFQfGITeY"
   },
   "source": [
    "**Loading the trained deep learning Model. This model is a binary classifier that predicts if the molecule is toxic or not.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0XvTQIMpITeZ",
    "outputId": "e9ff8c09-eac6-4b7f-a9f4-ca6d5e92e9cc"
   },
   "outputs": [],
   "source": [
    "def load_mymodel(jsonfile_path, weight_path):\n",
    "    '''\n",
    "    Return loaded model\n",
    "    Input:\n",
    "    jsonfile_path = path to model achitecture\n",
    "    weight_path: path to models trained weight\n",
    "    '''\n",
    "    json_file = open(jsonfile_path, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    p = json.loads(loaded_model_json)\n",
    "    model = model_from_json(p)\n",
    "    model.load_weights(weight_path)\n",
    "    return(model)\n",
    "\n",
    "\n",
    "def build_masked_loss(loss_function, mask_value):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    loss_function: The loss function to mask\n",
    "    mask_value: The value to mask in the targets\n",
    "\n",
    "    Returns:\n",
    "    function: a loss function that acts like loss_function with\n",
    "    masked inputs\n",
    "    \"\"\"\n",
    "\n",
    "    def masked_loss_function(y_true, y_pred):\n",
    "        mask = K.cast(K.not_equal(y_true, mask_value), K.floatx())\n",
    "\n",
    "        return loss_function(y_true * mask, y_pred * mask)\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "keras.losses.masked_loss_function = build_masked_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfile_path = \"../Final_tutorial/Model_architecture_Epoch14.json\"\n",
    "weight_path = \"../Final_tutorial/Model_weights_Epoch14.h5\"\n",
    "Final_model = load_mymodel(jsonfile_path , weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the Train and Test Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "# Training Data\n",
    "train  = pd.read_csv(\"../Final_tutorial/Ai_train_ToxAlerts_standardised_w_activity.csv\")\n",
    "train = train.drop([\"Clusters\", \"Fold\"], axis=1)\n",
    "inp_train = train.dropna(0)\n",
    "y_train = inp_train[\"Activity\"]\n",
    "ids_train = inp_train.Reference\n",
    "inp_train = inp_train.drop([\"Reference\", \"Activity\"], axis=1)\n",
    "features_name = list(inp_train)\n",
    "inp_train = inp_train.values\n",
    "\n",
    "#Test Data\n",
    "test = pd.read_csv(\"../Final_tutorial/Ai_test_ToxAlerts_standardised_w_activity_wo_trainingset_molecules.csv\")\n",
    "test = test.dropna(axis=0)\n",
    "preds = {}\n",
    "inp = test.dropna(0)\n",
    "y = inp[\"Activity\"]\n",
    "ids = inp.Reference\n",
    "inp = inp.drop([\"Reference\", \"Activity\"], axis=1)\n",
    "inp = inp.values\n",
    "pred = Final_model.predict(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgfqxltyITea"
   },
   "source": [
    "**Function to get the training distribution and neighbourhood data accordingly**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probablity of an observation to get sampled as a neighbourhood of a molecule depends on the distance. We assigned weights to each molecule. High weights if the model is close and low weights if it is far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T02:06:21.266606Z",
     "start_time": "2022-03-16T02:06:21.258789Z"
    }
   },
   "source": [
    "$$w_i = \\sqrt{ \\dfrac{exp(-d_i)}{wd^2}}$$\n",
    "\n",
    "where d is the distance between two points.\\\n",
    "wd is a scaling parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T02:14:03.066609Z",
     "start_time": "2022-03-16T02:14:03.060201Z"
    }
   },
   "source": [
    "We used jaccard similarity as a distance metrics\n",
    "$$Jaccard Similarity = \\dfrac{A\\cap B}{A\\cup B}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KI6GaG95ITea"
   },
   "outputs": [],
   "source": [
    "def get_training_distribution(training_data):\n",
    "    \"\"\"\n",
    "    We do the sampling of neighbourhood dataset according to training_data distribution. This function returns \n",
    "    columnwise fraction of 1s and 0s.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    training_data = numpy array of training data of shape (num_of_obs , num_of_features)\n",
    "    \n",
    "    Returns :\n",
    "    \n",
    "    feature_values = represents catagories present in every column, for bitwise vectors its always [0,1]\n",
    "    feature_frequency =represent the list of columnwise fraction of 1s and 0s.Returned in dictionary format where key is column number \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    feature_values = {}\n",
    "    feature_frequencies = {}\n",
    "    for feature in categorical_features:\n",
    "        column = training_data[:, feature]\n",
    "        feature_count = collections.Counter(column)\n",
    "        values, frequencies = map(list, zip(*(sorted(feature_count.items()))))\n",
    "        feature_values[feature] = values\n",
    "        feature_frequencies[feature] = np.round((np.array(frequencies) /\n",
    "                                             float(sum(frequencies)) ) ,decimals=2)\n",
    "    \n",
    "    return feature_values , feature_frequencies\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_neighbourhood_data(data_row,num_samples,feature_values,feature_frequencies,upsampling_parameter):\n",
    "    '''\n",
    "    Fits the fine tuned decision tree model on neighbourhood dataset.The final model is then fed into treeinterpreter \n",
    "    to calculate the feature importance for the test case\n",
    "    \n",
    "    Args : \n",
    "    \n",
    "    neighborhood_data = numpy array of neighbourhood dataset \n",
    "    labels_column = labels of neighborhood_data predicted from deep learning model \n",
    "    weights = weights according to jaccard distance. Far the molecule from the test case lower the weight and vice versa.\n",
    "    \n",
    "    \n",
    "    Return :\n",
    "    \n",
    "    easy_model = The best model after tuning min_samples_leaf\n",
    "    local_pred = Local prediction for the test case \n",
    "    scores = 5 folds cross validation scores\n",
    "    min_samples_leaf = The best value of min_samples_leaf after cross validation \n",
    "    \n",
    "    '''\n",
    "    num_cols = data_row.shape[0]\n",
    "    data = np.zeros((num_samples, num_cols))\n",
    "    first_row = data_row\n",
    "    data[0] = data_row.copy()\n",
    "    inverse = data.copy()\n",
    "\n",
    "    for column in categorical_features:\n",
    "        values = feature_values[column]\n",
    "        freqs = feature_frequencies[column]\n",
    "        #Few of the freqs have different shape \n",
    "        if freqs.shape==(1,):\n",
    "            freqs = np.asarray([0. ,1. ])\n",
    "            values = [0.0, 1.0]\n",
    "        #This part make sure that tweaking is around fixed bid only that is one \n",
    "        if first_row[column] ==0:\n",
    "            freqs = np.asarray([1. ,0. ])\n",
    "            \n",
    "        ##THis part will add the fucntionality of oversampling more ones.\n",
    "        freq_one = freqs[1]\n",
    "        freq_one=freq_one*(1+upsampling_parameter)\n",
    "        if freq_one > 1.0 : \n",
    "            freq_one = 1.0\n",
    "        freq_zero = 1.0 - freq_one\n",
    "        freqs=np.asarray([freq_zero,freq_one])\n",
    "        #######\n",
    "        random_state = column\n",
    "        random_state = check_random_state(random_state)\n",
    "        inverse_column = random_state.choice(values, size=num_samples,replace=True, p=freqs)\n",
    "\n",
    "        binary_column = np.array([1 if x == first_row[column]\n",
    "                                  else 0 for x in inverse_column])\n",
    "        binary_column[0] = 1\n",
    "        inverse_column[0] = data[0, column]\n",
    "        data[:, column] = binary_column\n",
    "        inverse[:, column] = inverse_column\n",
    "\n",
    "    inverse[0] = data_row\n",
    "    neighborhood_data = inverse\n",
    "    return neighborhood_data  \n",
    "\n",
    "\n",
    "def calculate_distances(dataset , datapoint):\n",
    "    '''\n",
    "    Calculates the jaccard distance between all the observations of dataset to datapoints.This is used to weigh the\n",
    "    neighbourhood observations according to test case(datapoint for the function)\n",
    "    \n",
    "    \n",
    "    Args : \n",
    "    \n",
    "    Dataset = A numpy array of shape (num_of_obs , num_of_features)\n",
    "    datapoint = A numpy array of shape (num_of_features, )\n",
    "    \n",
    "    Return:\n",
    "    \n",
    "    distances = Pairwise jaccard distance \n",
    "    '''\n",
    "    distances = sklearn.metrics.pairwise_distances(\n",
    "        dataset,\n",
    "        datapoint.reshape(1, -1),\n",
    "        metric='jaccard'\n",
    "    ).ravel()\n",
    "    \n",
    "    return distances \n",
    "\n",
    "\n",
    "\n",
    "def get_weights_from_distances(distances , scaling_factor):\n",
    "    '''\n",
    "    Calculates the weights from the distance. It is used as scaling tool where all the distances are converted between 0-1\n",
    "    The function that convert that is called kernel function and the formula is sqrt( exp(-distance)/kernel_width ** 2 ).\n",
    "    \n",
    "    Args :\n",
    "    \n",
    "    distance : Numpy array of distances.Its the output from calculate_distances \n",
    "    scaling_factor: to increase the gap between the weights ,should be around 35 for our dataset.\n",
    "    \n",
    "    '''\n",
    "    kernel_width = np.sqrt(training_data.shape[1]) * .75\n",
    "    kernel_width = float(kernel_width)\n",
    "    scaled_distance= distances*scaling_factor\n",
    "    weights=np.sqrt(np.exp(-(scaled_distance ** 2) / kernel_width ** 2)) \n",
    "    return weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def neighbourhood_model(neighborhood_data,labels_column,weights):\n",
    "    '''\n",
    "    Fits the fine tuned decision tree model on neighbourhood dataset.The final model is then fed into treeinterpreter \n",
    "    to calculate the feature importance for the test case\n",
    "    \n",
    "    Inputs : \n",
    "    \n",
    "    neighborhood_data = numpy array of neighbourhood dataset \n",
    "    labels_column = labels of neighborhood_data predicted from deep learning model \n",
    "    weights = weights according to jaccard distance. Far the molecule from the test case \n",
    "             lower the weight and vice versa.\n",
    "    \n",
    "    \n",
    "    Return :\n",
    "    \n",
    "    easy_model = The best model after tuning min_samples_leaf\n",
    "    local_pred = Local prediction for the test case \n",
    "    scores = 5 folds cross validation scores\n",
    "    min_samples_leaf = The best value of min_samples_leaf after cross validation \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    param_grid = {'min_samples_leaf' :[2,5,10,20,25,30,40,50,60,70] }#\n",
    "    easy_model = GridSearchCV(DecisionTreeRegressor(random_state= random_state),param_grid,cv = 5)#\n",
    "    easy_model.fit(neighborhood_data,\n",
    "                   labels_column, sample_weight=weights)\n",
    "    \n",
    "    best_param = easy_model.best_params_ #\n",
    "    easy_model = DecisionTreeRegressor(random_state= random_state,min_samples_leaf=best_param['min_samples_leaf'])\n",
    "    easy_model.fit(neighborhood_data,\n",
    "                   labels_column, sample_weight=weights)\n",
    "    residual = easy_model.score(\n",
    "        neighborhood_data,\n",
    "        labels_column, sample_weight=weights)\n",
    "    scores = cross_val_score(easy_model, neighborhood_data, labels_column, cv=5)\n",
    "    local_pred = easy_model.predict(neighborhood_data[0,:].reshape(1, -1))\n",
    "    y_pred= easy_model.predict(neighborhood_data)\n",
    "    \n",
    "\n",
    "    return easy_model,local_pred ,residual ,scores ,best_param['min_samples_leaf']\n",
    "\n",
    "\n",
    "def get_top3_sorted(contributions , feature_names):\n",
    "    '''\n",
    "    This function will sort the features/feature_combinations according to their absolute importance and returns \n",
    "    the top 3 important feature/feature_combinations.The features contributions are also scaled by dividing all \n",
    "    the contributions with sum of absolute value of contribution \n",
    "    \n",
    "    Args: \n",
    "    contributions = Contribution list which is the output from the tree interpreter \n",
    "    feature_names = Names of features in training dataset\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    final_dict = Dictionary containing the top 3 important features and their scaled contribution.\n",
    "    '''\n",
    "    current =contributions[0]\n",
    "    sorted_key =sorted(current, key=lambda k: abs(current[k]),reverse=True)\n",
    "    abs_sum = return_AbsSum(current)\n",
    "    current = {k: (v / abs_sum) for k, v in current.items()}\n",
    "    current = {sorted_key[0]:current[sorted_key[0]], sorted_key[1]:current[sorted_key[1]] ,sorted_key[2]:current[sorted_key[2]]}\n",
    "\n",
    "    #Propertly assigning the feature names according to training dataset \n",
    "\n",
    "    final_dict ={}\n",
    "    for key in current.keys():\n",
    "        change_key = []\n",
    "        for element in list(key):\n",
    "            element = feature_names[element]\n",
    "            change_key.append(element)\n",
    "        final_dict[tuple(change_key)] =current[key]\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "def absolute_sum(myDict): \n",
    "    '''\n",
    "    Calculates the sum of absolute value of feature_contribution. This is used to properly scale the feature contribution.\n",
    "    This function is automatically called in get_top3_sorted()  function and doesn't need to be called specifically.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    myDict = Sorted contribution dictionary  \n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    sum = return absolute sum of all the contribution.\n",
    "    '''\n",
    "\n",
    "    return sum(abs(myDict.values))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsXw8gdLITeb"
   },
   "source": [
    "**Input to the local model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQvctUxHITeb"
   },
   "source": [
    "With local model we are trying to understand how the decisions were made at local decision boundary. Here data row is the test example for which we are trying to find out the explainations. We need training data to get the distribution of 0s and 1s for each feature which will be further used to generate neighbourhood dataset. `predict_fn` will be used to get the training_data labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJD0L--BITec"
   },
   "outputs": [],
   "source": [
    "training_data=inp_train         # numpy array of training dataset without label.\n",
    "feature_names=features_name     #name of the features in the dataset in the list format\n",
    "categorical_features=list(range(training_data.shape[1]))\n",
    "predict_fn = Final_model.predict  #trained deep learning predict function ,should output the probability\n",
    "data_row=inp[5]       #test case \n",
    "num_samples = 5000   #Number of neighbourhood to be generated\n",
    "upsampling_parameter = 0.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejlwrTHVITec"
   },
   "source": [
    "**Get feature distribution from the training dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1bYaAldITed"
   },
   "source": [
    "While generating the neighbourhood dataset we will only tweak the features having value == 1 for the test case. So the feature with value ==0 will stay zero for all the generated neighbouhood dataponts and will be of no importance for local level interpretable model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEZifv4CITed"
   },
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "random_state = check_random_state(random_state)\n",
    "feature_values , feature_frequencies = get_training_distribution(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T02:27:17.632415Z",
     "start_time": "2022-03-16T02:27:17.627852Z"
    },
    "id": "WHkVUTRBITed",
    "outputId": "57ed288a-ee47-401c-b2c0-01ae212ebc06"
   },
   "outputs": [],
   "source": [
    "#feature_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OR2ACXemITed"
   },
   "source": [
    "**Generate the neighbourhood dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfyUYAL6ITed"
   },
   "outputs": [],
   "source": [
    "neighborhood_data=get_neighbourhood_data(data_row,num_samples,feature_values,feature_frequencies,upsampling_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LS-y_KALITed",
    "outputId": "885b9ff0-675c-4339-987e-22ed9e301592"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighborhood_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofpzgD7rITee"
   },
   "source": [
    "**Calculates the distance of test case from training dataset. Then find that 22% of that maximum distance and call it as r_fidelity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfUcXvEYITee"
   },
   "source": [
    "To check how accurately our interpretable model is capturing the local decision boundary we will check the R2(r-squared) scored in a circle of radius 0.22*r_max ,where r_max is the maximum distance of test case from the training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAyPp1QmITee",
    "outputId": "c1c35bf0-79d7-4e9f-9682-e00185c2f09b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20533333333333334\n"
     ]
    }
   ],
   "source": [
    "distances_train=calculate_distances(training_data , neighborhood_data[0])\n",
    "r_fidelity =0.22*max(distances_train)\n",
    "print(r_fidelity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WIUCy1KITee"
   },
   "source": [
    "**Distance of test case from all the generated neighbourhood samples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRRWPb09ITee"
   },
   "source": [
    "We will weigh the neighbourhood datapoints according to the test case,so that to be in local space the points that are closer will be given high weight and vice versa.The following distance calculates the jaccard distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UaBOkVBxITee",
    "outputId": "991dd6ed-5900-4dfe-8b2d-3a2d453ef0e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted distances are : [0.         0.66666667 0.66666667 ... 0.68888889 0.68888889 0.64444444]\n"
     ]
    }
   ],
   "source": [
    "distances = calculate_distances(neighborhood_data , neighborhood_data[0])\n",
    "print('sorted distances are :', distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByJOHomTITee"
   },
   "source": [
    "As you can see the sorted list the distances are very closer so we multiplied the distance with scaling_factor so that the weights are properly separeted.This step should to properly checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VccXqSY9ITee",
    "outputId": "226583b1-0496-4753-ec1f-33882a6dcbab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.638574   0.638574   ... 0.61945374 0.61945374 0.65762864]\n",
      "sorted weights to check the separation : [0.54308879 0.54308879 0.54308879 ... 0.8192703  0.8192703  1.        ]\n"
     ]
    }
   ],
   "source": [
    "scaling_factor = 35\n",
    "weights=get_weights_from_distances(distances , scaling_factor)\n",
    "\n",
    "print(weights)\n",
    "print('sorted weights to check the separation :',np.sort(weights) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENjgKkcqITee"
   },
   "source": [
    "As you can see the weights are now from 0.63 to 0.81 for this case and are well separated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WP-OO6ZOITef"
   },
   "source": [
    "**Get the predictions from the deep learning model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9EPpMeYITef"
   },
   "source": [
    "We will predict the output for neighbourhood dataset using deep learning model and these output will be used as labels for our local model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_aBxzhNITef"
   },
   "outputs": [],
   "source": [
    "neighborhood_labels = predict_fn(neighborhood_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJOvFaJ_ITef"
   },
   "source": [
    "**Build the interpretable model at the local space**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4BH54zuITef"
   },
   "source": [
    "With generated neighbourhood data as new training data for local model, deep learning prediction for this data as the label and weighed by the distance we built a fine tuned local interpretable model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4BvQs98cITef"
   },
   "outputs": [],
   "source": [
    "labels_column = neighborhood_labels[:, 0] \n",
    "easy_model,local_pred ,residual ,scores ,min_samples_leaf= neighbourhood_model(neighborhood_data,\n",
    "                                                                               labels_column,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVbHyUeGITef",
    "outputId": "b51e6a0b-3762-40a5-a79c-932854f8c1b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual of the best model is:   0.8368967751403357\n",
      "Local prediction of the model is:   [0.44859488]\n",
      "Right prediction of the model is:  0.27391064\n",
      "Five fold cross validation scores are:  [0.61584687 0.61524735 0.59794527 0.63370461 0.61151395]\n",
      "Best min sample leaf is:  5\n"
     ]
    }
   ],
   "source": [
    "print('Residual of the best model is:  ', residual)\n",
    "print('Local prediction of the model is:  ',local_pred)\n",
    "print('Right prediction of the model is: ',labels_column[0] )\n",
    "print('Five fold cross validation scores are: ',scores)\n",
    "print('Best min sample leaf is: ' , min_samples_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVg5R9fkITef"
   },
   "source": [
    "**Getting the feature importance through tree interpreter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJtxXV22ITef"
   },
   "source": [
    "tree interpreter represents prediction = `bias + feature_1_contribution +feature_2_contribution+ all possible pairs.` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6oQRIGKITef"
   },
   "source": [
    "So the negative value of feature contribution is trying to pull prediction towards zero and positive value in contribution is trying to push the rediction towards 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XAzPbsQkITef"
   },
   "outputs": [],
   "source": [
    "prediction, bias, contributions = ti.predict(easy_model, neighborhood_data[0,:].reshape(1,-1),joint_contribution=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buPNZl_UITef",
    "outputId": "ba503f1c-0952-4068-9f74-dede837698cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{(6,): array([-0.09581262]),\n",
       "  (6, 15): array([-0.03536325]),\n",
       "  (6, 15, 40): array([0.18496529]),\n",
       "  (6, 15, 40, 177): array([-0.25022517]),\n",
       "  (6, 15, 40, 71, 177): array([-0.06499414])}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpYPPObZITef"
   },
   "outputs": [],
   "source": [
    "final_dict=get_top3_sorted(contributions , feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z99UkvqiITef",
    "outputId": "a7f3460f-6be5-4bbd-b58f-7a4f18d068ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('TA1095', 'TA1301', 'TA390', 'TA9503'): array([-0.39632696]),\n",
       " ('TA1095', 'TA1301', 'TA390'): array([0.29296306]),\n",
       " ('TA1095',): array([-0.15175581])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nbk3q5hMITeg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Tutorial Notebook .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
